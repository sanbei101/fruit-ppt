---
theme: seriph
title: å›¾åƒè¯†åˆ«ä»»åŠ¡ -- æœè”¬åˆ†ç±»
background: /background.png
class: text-center
drawings:
  persist: false
transition: slide-left
mdc: true
---

# å›¾åƒè¯†åˆ«ä»»åŠ¡

# æœè”¬åˆ†ç±» è¯¾ç¨‹ç­”è¾©

---
layout: two-cols
layoutClass: gap-16
---

# ä»»åŠ¡æ¦‚è¿°

**æ•°æ®é›†æ¥æº**: Kaggle Fruits 360

<div class="text-sm">

| é¡¹ç›®       | å†…å®¹        |
| ---------- | ----------- |
| æ€»å›¾ç‰‡æ•°   | 90,483 å¼    |
| åŸå§‹ç±»åˆ«   | 131 ç§æœè”¬  |
| å›¾ç‰‡å°ºå¯¸   | 100Ã—100 RGB |
| æœ¬ä»»åŠ¡ç±»åˆ« | 10 ç±»       |

</div>

**é€‰å®šç±»åˆ«**:
è‹¹æœã€é¦™è•‰ã€æ©™å­ã€è‰è“ã€è¥¿çº¢æŸ¿ã€é»„ç“œã€èŒ„å­ã€è‘¡è„ã€èŠ’æœã€è¥¿ç“œ

::right::
# æ•°æ®é›†ç¤ºä¾‹

<div class="grid grid-cols-5 gap-2 pt-4">
  <div><img src="/fruit/apple.jpg" class="h-20 rounded shadow"></div>
  <div><img src="/fruit/banana.jpg" class="h-20 rounded shadow"></div>
  <div><img src="/fruit/orange.jpg" class="h-20 rounded shadow"></div>
  <div><img src="/fruit/strawberry.jpg" class="h-20 rounded shadow"></div>
  <div><img src="/fruit/tomato.jpg" class="h-20 rounded shadow"></div>
  <div><img src="/fruit/cucumber.jpg" class="h-20 rounded shadow"></div>
  <div><img src="/fruit/eggplant.jpg" class="h-20 rounded shadow"></div>
  <div><img src="/fruit/grape.jpg" class="h-20 rounded shadow"></div>
  <div><img src="/fruit/mango.jpg" class="h-20 rounded shadow"></div>
  <div><img src="/fruit/watermelon.jpg" class="h-20 rounded shadow"></div>
</div>

<div class="pt-4 text-sm">

```python
# æ£€æµ‹åˆ°çš„ç±»åˆ«æ€»æ•°
num_classes = 10
class_names = ['apple', 'banana', 'orange',
               'strawberry', 'tomato', 'cucumber',
               'eggplant', 'grape', 'mango',
               'watermelon']
```
</div>

---
layout: two-cols
layoutClass: gap-16
---

# æ•°æ®é¢„å¤„ç†

**è®­ç»ƒé›†å¢å¼º**:

```python
train_transform = transforms.Compose([
    transforms.Resize((100, 100)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(20),
    transforms.ToTensor(),
    transforms.Normalize(
        [0.485, 0.456, 0.406],
        [0.229, 0.224, 0.225]
    )
])
```

**æµ‹è¯•é›†æ ‡å‡†åŒ–**:

```python
test_transform = transforms.Compose([
    transforms.Resize((100, 100)),
    transforms.ToTensor(),
    transforms.Normalize(
        [0.485, 0.456, 0.406],
        [0.229, 0.224, 0.225]
    )
])
```

::right::

# æ•°æ®åŠ è½½

**æ•°æ®åŠ è½½é…ç½®**:

```python
from torch.utils.data import DataLoader
train_loader = DataLoader(
    full_train_dataset,
    batch_size=256,
    shuffle=True
)
test_loader = DataLoader(
    full_test_dataset,
    batch_size=256,
    shuffle=False
)
```

**ç¯å¢ƒé…ç½®**:

```python
device = torch.device(
    "cuda" if torch.cuda.is_available() else "cpu"
)
# å½“å‰ä½¿ç”¨çš„è®¾å¤‡: cuda
```

---
layout: two-cols
layoutClass: gap-16
---

# æ¨¡å‹ 1: Simple CNN

**ç½‘ç»œç»“æ„**:

```python
model_cnn = nn.Sequential(
    nn.Conv2d(3, 16, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2, 2),
    nn.Conv2d(16, 32, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2, 2),
    nn.Flatten(),
    nn.Linear(32*25*25, 256),
    nn.ReLU(),
    nn.Linear(256, num_classes)
).to(device)
```

**ä¼˜åŒ–å™¨é…ç½®**:

```python
criterion = nn.CrossEntropyLoss()
optimizer_cnn = optim.Adam(
    model_cnn.parameters(),
    lr=0.001
)
```

::right::

# CNN è®­ç»ƒè¿‡ç¨‹

**è®­ç»ƒå¾ªç¯**:

```python
for epoch in range(5):
    model_cnn.train()
    running_loss = 0.0
    for images, labels in train_loader:
        optimizer_cnn.zero_grad()
        outputs = model_cnn(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer_cnn.step()
        running_loss += loss.item()
```

é‡‡ç”¨åºåˆ—åŒ–è®¾è®¡ï¼Œé€šè¿‡ä¸¤å±‚å·ç§¯-æ¿€æ´»-æ± åŒ–çš„ç»å…¸ç»„åˆï¼Œé€æ­¥æå–å¹¶å‹ç¼©å›¾åƒç‰¹å¾ï¼Œæœ€ç»ˆç”±å…¨è¿æ¥å±‚å®Œæˆåˆ†ç±»

ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°è¡¡é‡é¢„æµ‹è¯¯å·®ï¼Œå¹¶é…ç½®Adamä¼˜åŒ–å™¨è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡

---
layout: two-cols
layoutClass: gap-16
---

# CNN è®­ç»ƒç»“æœ
### ç®€å• CNN åœ¨ 5 è½®å†…å®ç°å¿«é€Ÿæ”¶æ•›

**è®­ç»ƒæ•°æ®**:

<div class="text-sm">

| Epoch | Loss   |
| ----- | ------ |
| 1     | 0.9560 |
| 2     | 0.1175 |
| 3     | 0.0413 |
| 4     | 0.0209 |
| 5     | 0.0118 |

</div>

::right::
# Loss å˜åŒ–æ›²çº¿

```mermaid {scale: 0.6}
xychart-beta
    title "Training Loss"
    x-axis [1, 2, 3, 4, 5]
    y-axis "Loss" 0 --> 1
    line [0.956, 0.118, 0.041, 0.021, 0.012]
```
**æ”¶æ•›åˆ†æ**:
- Epoch 1â†’2: Loss å¤§å¹…ä¸‹é™ 87.7%
- Epoch 2â†’3: ç»§ç»­å¿«é€Ÿä¸‹é™
- Epoch 3â†’5: é€æ¸æ”¶æ•›è‡³ 0.0118

---
layout: two-cols
layoutClass: gap-16
---

# æ¨¡å‹ 2: ResNet

**è¿ç§»å­¦ä¹ é…ç½®**:

```python
weights = ResNet18_Weights.DEFAULT
model_resnet = resnet18(weights=weights)
model_resnet.fc = nn.Linear(
    model_resnet.fc.in_features,
    num_classes
)
model_resnet = model_resnet.to(device)
```

**ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦**:

```python
optimizer_res = optim.Adam(
    model_resnet.parameters(),
    lr=0.0001
)
scheduler = optim.lr_scheduler.StepLR(
    optimizer_res,
    step_size=5,
    gamma=0.1
)
```

::right::

# ResNet è®­ç»ƒè¿‡ç¨‹

**å¼ºåŒ–è®­ç»ƒå¾ªç¯**:
```python
for epoch in range(10):
    model_resnet.train()
    running_loss = 0.0
    correct, total = 0, 0
    for images, labels in train_loader:
        optimizer_res.zero_grad()
        outputs = model_resnet(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer_res.step()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    scheduler.step()
    acc = 100 * correct / total
    print(f"Epoch {epoch+1}, Accuracy: {acc:.2f}%")
```
<div class="text-sm">

åŠ è½½ `ImageNet` é¢„è®­ç»ƒæƒé‡ï¼Œä»…æ›¿æ¢æœ€åçš„å…¨è¿æ¥å±‚ä»¥é€‚åº” 10 åˆ†ç±»ä»»åŠ¡ï¼Œ
ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡ `0.0001` å¾®è°ƒï¼Œé…åˆ `StepLR` è°ƒåº¦å™¨æ¯ 5 è½®è¡°å‡å­¦ä¹ ç‡ã€‚

</div>

---
layout: two-cols
layoutClass: gap-16
---

# ResNet18 è®­ç»ƒç»“æœ
### è¿ç§»å­¦ä¹ åœ¨ç¬¬ 2 è½®å³è¾¾åˆ°å‡ ä¹ 100% å‡†ç¡®ç‡

**è®­ç»ƒæ•°æ®**:

<div class="text-sm">

| Epoch | Accuracy (%) | Learning Rate |
| ----- | ------------ | ------------- |
| 1     | 92.87        | 0.0001        |
| 2     | 100.00       | 0.0001        |
| 3-5   | 100.00       | 0.0001        |
| 6-10  | 100.00       | 1e-05 ~ 1e-06 |

</div>

::right::

# Accuracy å˜åŒ–æ›²çº¿

```mermaid {scale: 0.6}
xychart-beta
    title "Training Accuracy"
    x-axis [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    y-axis "Accuracy (%)" 90 --> 100
    line [92.87, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]
```
**è¿ç§»å­¦ä¹ ä¼˜åŠ¿**:
- Epoch 1: è¿…é€Ÿè¾¾åˆ° 92.87%
- Epoch 2: å®Œç¾æ”¶æ•›è‡³ 100%
- åç»­è½®æ¬¡: ä¿æŒç¨³å®š

---
layout: two-cols
layoutClass: gap-16
---

# æ¨¡å‹å¯¹æ¯”

<div class="text-sm">

| å¯¹æ¯”é¡¹ | Simple CNN | ResNet18 |
| -------------- | ---------- | ------------ |
| **å‚æ•°é‡** | çº¦ 500K | çº¦ 11M |
| **è®­ç»ƒè½®æ•°** | 5 | 10 |
| **å­¦ä¹ ç‡** | 0.001 | 0.0001 |
| **æœ€ç»ˆ Loss** | 0.0118 | ~0 |
| **è®­ç»ƒå‡†ç¡®ç‡** | ~99% | 100% |
| **è®­ç»ƒæ—¶é—´** | è¾ƒçŸ­ | è¾ƒé•¿ |

</div>

<div class="pt-4 text-sm text-yellow-600">

**ç»“è®º**: ResNet18 å‡­å€Ÿè¿ç§»å­¦ä¹ ä¼˜åŠ¿ï¼Œåœ¨ç¬¬ 2 è½®å³è¾¾åˆ° 100%è®­ç»ƒå‡†ç¡®ç‡

</div>

::right::

# æ¨¡å‹è¯„ä¼°ä»£ç 

**æµ‹è¯•é›†è¯„ä¼°**:

```python
model_resnet.eval()
all_preds = []
all_labels = []
with torch.no_grad():
    for images, labels in test_loader:
        outputs = model_resnet(images)
        _, predicted = torch.max(outputs, 1)
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        print(classification_report(
              all_labels, all_preds,
              target_names=class_names
        ))
```

**å¯è§†åŒ–é¢„æµ‹**:

```python
images, labels = next(iter(test_loader))
outputs = model_resnet(images)
_, preds = torch.max(outputs, 1)
```

---
layout: two-cols
layoutClass: gap-16
---

# æµ‹è¯•ç»“æœè¯„ä¼°

<div class="text-sm">

| ç±»åˆ« | Precision | Recall | F1-Score | Support |
| ------ | --------- | ------ | -------- | ------- |
| apple | 0.99 | 0.97 | 0.98 | 1000 |
| banana | 0.98 | 0.99 | 0.98 | 645 |
| cucumber | 0.97 | 0.96 | 0.96 | 1000 |
| eggplant | 0.96 | 0.98 | 0.97 | 236 |
| grape | 0.95 | 0.94 | 0.94 | 1000 |
| mango | 0.98 | 0.99 | 0.98 | 410 |
| orange | 0.99 | 0.95 | 0.97 | 160 |
| strawberry | 1.00 | 1.00 | 1.00 | 410 |
| tomato | 0.97 | 0.98 | 0.97 | 1000 |
| watermelon | 1.00 | 1.00 | 1.00 | 157 |

</div>

::right::
**æ€»ä½“å‡†ç¡®ç‡: 98%**

## è‹¹æœé¢„æµ‹ç¤ºä¾‹

```python
for i in range(5):
    plt.subplot(1, 5, i+1)
    img = images[i].cpu().numpy()
    # åå½’ä¸€åŒ–
    img = np.clip(np.array([0.229, 0.224, 0.225]) 
    * img + np.array([0.485, 0.456, 0.406]), 0, 1)
    plt.imshow(img)
    plt.title(f"Pred: {class_names[preds[i]]}\n 
                True: {class_names[labels[i]]}")
    plt.axis('off')
plt.show()
```

æ­£ç¡®è¯†åˆ«è‹¹æœ
<img src="/test-apple.png">

---
layout: two-cols
layoutClass: gap-16
---

# è¯¯å·®åˆ†æ

<div class="text-sm">

| çœŸå®ç±»åˆ« | ä¸»è¦è¯¯åˆ¤ä¸º | åŸå› åˆ†æ |
|---------|-----------|---------|
| Apple | Mango, Tomato | çº¢è‰²/æ©™è‰²å“ç§é¢œè‰²ç›¸è¿‘ |
| Grape | Orange | å…‰ç…§åå°„å¯¼è‡´é¢œè‰²åå·® |
| Orange | Mango | æ©™è‰²ç³»æ°´æœç‰¹å¾ç›¸ä¼¼ |
| Cucumber | Banana | é•¿æ¡å½¢è½®å»“æ˜“æ··æ·† |


</div>

::right::

# æ”¹è¿›æ–¹å‘

**æ•°æ®å±‚é¢**:

<div class="text-sm">

**å›°éš¾æ ·æœ¬æŒ–æ˜**
- æ”¶é›†æ˜“æ··æ·†å“ç§çš„è¾¹ç•Œæ ·æœ¬
- å¢åŠ çº¢è‰²/æ©™è‰²ç³»æ•°æ®çš„å¤šæ ·æ€§

</div>

**æ¨¡å‹å±‚é¢**:

<div class="text-sm">

**æ›´å¤§é¢„è®­ç»ƒæ¨¡å‹**
- ResNet50/EfficientNet
- æ›´å¤šå±‚æ¬¡çš„ç‰¹å¾æå–

</div>

---
layout: two-cols
layoutClass: gap-16
---

# ONNX æ¨¡å‹å¯¼å‡º

**å¯¼å‡ºé…ç½®**:

```python
import torch

# å‡†å¤‡è™šæ‹Ÿè¾“å…¥
dummy_input = torch.randn(1, 3, 100, 100).to(device)

# åŠ¨æ€ batch é…ç½®
batch = torch.export.Dim("batch", min=1, max=1024)
dynamic_shapes = {"x": {0: batch}}
```

<div class="text-sm">

**å¯¼å‡ºå‚æ•°è¯´æ˜**:

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `opset_version` | 21 | ONNX ç®—å­é›†ç‰ˆæœ¬ |
| `do_constant_folding` | True | å¸¸é‡æŠ˜å ä¼˜åŒ– |
| `dynamic_shapes` | batch=1~1024 | æ”¯æŒåŠ¨æ€æ‰¹æ¬¡ |

</div>

::right::

# æ¨¡å‹å¯¼å‡ºä»£ç 

```python
torch.onnx.export(
    model_resnet,
    dummy_input,
    "fruit_model.onnx",
    export_params=True,
    opset_version=21,
    do_constant_folding=True,
    input_names=['x'],
    output_names=['output'],
    dynamic_shapes=dynamic_shapes,
    dynamo=True
)
```

<div class="text-sm pt-4">

**ä¸ºä»€ä¹ˆé€‰æ‹© ONNX?**

- **è·¨å¹³å°éƒ¨ç½²**: ä¸€æ¬¡å¯¼å‡ºï¼Œåˆ°å¤„è¿è¡Œ
- **æ¨ç†ä¼˜åŒ–**: ONNXRuntime é’ˆå¯¹ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«
- **æ¡†æ¶æ— å…³**: è§£è€¦ PyTorch ä¾èµ–,æ— éœ€åœ¨å®¢æˆ·ç«¯å®‰è£…å®Œæ•´æ·±åº¦å­¦ä¹ æ¡†æ¶

</div>

---
layout: two-cols
layoutClass: gap-16
---

# Gradio å¯è§†åŒ–éƒ¨ç½²

**éƒ¨ç½²æ–¹æ¡ˆ**:

```python
import gradio as gr
import onnxruntime as ort
import numpy as np

session = ort.InferenceSession(
    "fruit_model.onnx",
    providers=["CPUExecutionProvider"]
)
targets = ["apple", "banana", "orange",
           "strawberry", "tomato", "cucumber",
           "eggplant", "grape", "mango",
           "watermelon"]
```

<div class="text-sm pt-4">

**æ ¸å¿ƒä¼˜åŠ¿**:
- ONNX æ¨¡å‹æ¨ç†,è·¨å¹³å°å…¼å®¹
- CPU æ‰§è¡Œ,æ— éœ€ GPU
- Gradio ä¸€é”®ç”Ÿæˆ Web ç•Œé¢

</div>

::right::

# é¢„å¤„ç†é€»è¾‘

```python
def predict(img):
    # è½¬æ¢ä¸º RGB å¹¶è°ƒæ•´å°ºå¯¸
    img = img.convert("RGB").resize((100, 100))
    img_data = np.array(img).transpose(2, 0, 1)
    img_data = img_data.astype(np.float32) / 255.0

    # ImageNet æ ‡å‡†åŒ–
    mean, std = [0.485, 0.456, 0.406], 
                [0.229, 0.224, 0.225]
    for i in range(3):
       img_data[i] = (img_data[i] - mean[i]) / std[i]

    # ONNX æ¨ç†
    outputs = session.run(
        ["output"],
        {"x": img_data[None, ...]}
    )

    # Softmax æ¦‚ç‡è®¡ç®—
    exp_out = np.exp(outputs[0][0])
    probs = exp_out / np.sum(exp_out)
    return {targets[i]: float(probs[i])
            for i in range(len(targets))}
```

---
layout: two-cols
layoutClass: gap-16
---

# Gradio ç•Œé¢é…ç½®

```python
demo = gr.Interface(
    fn=predict,
    inputs=gr.Image(type="pil", label="ä¸Šä¼ æ°´æœå›¾ç‰‡"),
    outputs=gr.Label(
        num_top_classes=3,
        label="é¢„æµ‹ç»“æœ"
    ),
    title="ğŸ æ°´æœå›¾åƒåˆ†ç±»å™¨",
    description="ä¸Šä¼ ä¸€å¼ æ°´æœå›¾ç‰‡ï¼Œæ¨¡å‹å°†è‡ªåŠ¨è¯†åˆ«å…¶ç§ç±»",
    examples=[
        ["example/apple.jpg"],
        ["example/banana.jpg"],
        ["example/strawberry.jpg"],
        ["example/tomato.jpg"],
        ["example/eggplant.jpg"],
        ["example/grape.jpg"],
        ["example/mango.jpg"],
        ["example/watermelon.jpg"],
    ],
)
if __name__ == "__main__":
    demo.launch(share=True)
```

::right::
# ç•Œé¢é¢„è§ˆ

<div>
  <img src="/predict-1.png">

  <img src="/predict-2.png" class="pt-4">
</div>

---
layout: center
class: text-center
---

# æ„Ÿè°¢è†å¬


<div class="pt-8">
  <img src="/fruit/strawberry.jpg" class="h-32 rounded shadow mx-auto">
</div>
